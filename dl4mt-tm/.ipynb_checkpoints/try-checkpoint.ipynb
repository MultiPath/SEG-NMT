{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 0: GeForce GTX 1080 (CNMeM is disabled, cuDNN 5105)\n",
      "/usr/local/lib/python2.7/dist-packages/theano/sandbox/cuda/__init__.py:600: UserWarning: Your cuDNN version is more recent than the one Theano officially supports. If you see any problems, try updating Theano or downgrading cuDNN to version 5.\n",
      "  warnings.warn(warn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'baseline_ef': '/root/disk/scratch/model-tmnmt/baseline_enfr.bs64.npz',\n",
      " 'baseline_fe': '/root/disk/scratch/model-tmnmt/baseline_fren.npz',\n",
      " 'batch_size': 16,\n",
      " 'beamsize': 5,\n",
      " 'clip_c': 1.0,\n",
      " 'd_maxlen': 200,\n",
      " 'datasets': ['/root/workspace/TMNMT/.dataset/fren/train.fr.tok.shuf',\n",
      "              '/root/workspace/TMNMT/.dataset/fren/train.en.tok.shuf',\n",
      "              '/root/workspace/TMNMT/.dataset/fren/train.fr.tok.shuf',\n",
      "              '/root/workspace/TMNMT/.dataset/fren/train.en.tok.shuf'],\n",
      " 'decay_c': 0.0,\n",
      " 'decoder': 'gru_cond',\n",
      " 'dictionaries': ['/root/workspace/TMNMT/.dataset/fren/train.fr.tok.pkl',\n",
      "                  '/root/workspace/TMNMT/.dataset/fren/train.en.tok.pkl',\n",
      "                  '/root/workspace/TMNMT/.dataset/fren/train.fr.tok.pkl',\n",
      "                  '/root/workspace/TMNMT/.dataset/fren/train.en.tok.pkl'],\n",
      " 'dim': 1024,\n",
      " 'dim_word': 512,\n",
      " 'dispFreq': 10,\n",
      " 'encoder': 'gru',\n",
      " 'lrate': 2e-05,\n",
      " 'maxlen': 50,\n",
      " 'normalize': False,\n",
      " 'optimizer': 'adam',\n",
      " 'overwrite': True,\n",
      " 'patience': 1000,\n",
      " 'reload_': True,\n",
      " 'sampleFreq': 20,\n",
      " 'saveFreq': 500,\n",
      " 'saveto': '/root/disk/scratch/model-tmnmt/tmv3_fren.ff.16-50.npz',\n",
      " 'stochastic': True,\n",
      " 'trans_from': '/root/workspace/TMNMT/.dataset/fren/devset.fr.tok',\n",
      " 'trans_ref': '/root/workspace/TMNMT/.dataset/fren/devset.en.tok',\n",
      " 'trans_to': '/root/workspace/TMNMT/.translate/tmv3_',\n",
      " 'use_dropout': False,\n",
      " 'use_pretrain': True,\n",
      " 'validFreq': 100,\n",
      " 'valid_batch_size': 32,\n",
      " 'valid_datasets': ['/root/workspace/TMNMT/.dataset/fren/devset.fr.tok',\n",
      "                    '/root/workspace/TMNMT/.dataset/fren/devset.en.tok',\n",
      "                    '/root/workspace/TMNMT/.dataset/fren/devset.fr.tok',\n",
      "                    '/root/workspace/TMNMT/.dataset/fren/devset.en.tok'],\n",
      " 'voc_sizes': [20000, 20000, 20000, 20000]}\n",
      "Building model: E -> F & F -> E model\n",
      "Done.\n",
      "build forward-attention models (4 models simultaneously)...\n",
      "Build f_critic... Done\n",
      "Build f_critic... Done\n",
      "build cross-attention models\n",
      "build attentions (forward, cross-propagation)\n",
      "build gates!\n",
      "Building a Natural Gate Function\n",
      "build loss function (w/o gate)\n",
      "build sampler (one-step)\n",
      "Building f_init... Done\n",
      "Building f_next... Done\n",
      "Building f_init... Done\n",
      "Building f_next... Done\n",
      "build attender (one-step)\n",
      "Build f_attend... Done.\n",
      "Build f_attend... Done.\n",
      "build Cost Function... build Gradient (backward)... Done\n",
      "Building Optimizers... Done\n",
      "Build Networks... done!\n",
      "build_networks: elapsed 841.5808 secs.\n",
      "\n",
      "Loading data\n",
      "use the pretrained NMT-models... load ... ef_ff_state_W\n",
      "load ... ef_decoder_W_comb_att\n",
      "load ... ef_Wemb_dec\n",
      "load ... ef_decoder_U_att\n",
      "load ... ef_encoder_r_U\n",
      "load ... ef_encoder_r_W\n",
      "load ... ef_ff_logit_b\n",
      "load ... ef_encoder_bx\n",
      "load ... ef_encoder_r_Ux\n",
      "load ... ef_decoder_bx\n",
      "load ... ef_decoder_b_nl\n",
      "load ... ef_decoder_Ux\n",
      "load ... ef_ff_state_b\n",
      "load ... ef_encoder_Ux\n",
      "load ... ef_encoder_r_Wx\n",
      "load ... ef_decoder_bx_nl\n",
      "load ... ef_ff_logit_W\n",
      "load ... ef_encoder_r_b\n",
      "load ... ef_decoder_Wx\n",
      "load ... ef_ff_logit_lstm_W\n",
      "load ... ef_ff_logit_prev_b\n",
      "load ... ef_ff_logit_ctx_b\n",
      "load ... ef_decoder_Wcx\n",
      "load ... ef_decoder_b\n",
      "load ... ef_encoder_U\n",
      "load ... ef_decoder_Wc\n",
      "load ... ef_encoder_W\n",
      "load ... ef_decoder_b_att\n",
      "load ... ef_decoder_Wc_att\n",
      "load ... ef_decoder_U_nl\n",
      "load ... ef_decoder_U\n",
      "load ... ef_decoder_W\n",
      "load ... ef_encoder_r_bx\n",
      "load ... ef_decoder_c_tt\n",
      "load ... ef_encoder_b\n",
      "load ... ef_ff_logit_lstm_b\n",
      "load ... ef_decoder_Ux_nl\n",
      "load ... ef_Wemb\n",
      "load ... ef_encoder_Wx\n",
      "load ... ef_ff_logit_ctx_W\n",
      "load ... ef_ff_logit_prev_W\n",
      "load ... fe_ff_logit_prev_W\n",
      "load ... fe_decoder_Wc_att\n",
      "load ... fe_encoder_U\n",
      "load ... fe_Wemb_dec\n",
      "load ... fe_ff_state_b\n",
      "load ... fe_ff_logit_ctx_b\n",
      "load ... fe_encoder_r_W\n",
      "load ... fe_encoder_r_bx\n",
      "load ... fe_ff_logit_lstm_W\n",
      "load ... fe_encoder_Ux\n",
      "load ... fe_encoder_Wx\n",
      "load ... fe_ff_logit_ctx_W\n",
      "load ... fe_decoder_b_att\n",
      "load ... fe_decoder_b_nl\n",
      "load ... fe_ff_state_W\n",
      "load ... fe_decoder_U_nl\n",
      "load ... fe_decoder_Ux_nl\n",
      "load ... fe_ff_logit_prev_b\n",
      "load ... fe_decoder_bx\n",
      "load ... fe_encoder_r_b\n",
      "load ... fe_ff_logit_W\n",
      "load ... fe_decoder_c_tt\n",
      "load ... fe_encoder_b\n",
      "load ... fe_encoder_r_U\n",
      "load ... fe_decoder_U_att\n",
      "load ... fe_decoder_Ux\n",
      "load ... fe_decoder_b\n",
      "load ... fe_Wemb\n",
      "load ... fe_decoder_W\n",
      "load ... fe_encoder_bx\n",
      "load ... fe_decoder_Wcx\n",
      "load ... fe_decoder_U\n",
      "load ... fe_encoder_r_Ux\n",
      "load ... fe_decoder_bx_nl\n",
      "load ... fe_decoder_Wx\n",
      "load ... fe_decoder_W_comb_att\n",
      "load ... fe_decoder_Wc\n",
      "load ... fe_encoder_W\n",
      "load ... fe_encoder_r_Wx\n",
      "load ... fe_ff_logit_lstm_b\n",
      "load ... fe_ff_logit_b\n",
      "Done.\n",
      "-------------------------------------------- Main-Loop -------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# %load train_nmt.py\n",
    "from nmt import *\n",
    "from pprint import pprint\n",
    "from setup import setup\n",
    "from data_iterator import TextIterator, prepare_data, prepare_cross\n",
    "\n",
    "# import argparse\n",
    "\n",
    "# parser = argparse.ArgumentParser()\n",
    "# parser.add_argument('-m', type=str, default='fren')\n",
    "# args = parser.parse_args()\n",
    "\n",
    "model_options = setup('fren')\n",
    "pprint(model_options)\n",
    "\n",
    "# add random seed\n",
    "model_options['rng']  = numpy.random.RandomState(seed=19920206)\n",
    "model_options['trng'] = RandomStreams(model_options['rng'].randint(0, 2**32-1))\n",
    "model_options['n_words_src'] = model_options['voc_sizes'][0]\n",
    "model_options['n_words'] = model_options['voc_sizes'][1]\n",
    "\n",
    "\n",
    "# load dictionaries and invert them\n",
    "worddicts   = [None] * len(model_options['dictionaries'])\n",
    "worddicts_r = [None] * len(model_options['dictionaries'])\n",
    "for ii, dd in enumerate(model_options['dictionaries']):\n",
    "    with open(dd, 'rb') as f:\n",
    "        worddicts[ii] = pkl.load(f)\n",
    "    worddicts_r[ii] = dict()\n",
    "    for kk, vv in worddicts[ii].iteritems():\n",
    "        worddicts_r[ii][vv] = kk\n",
    "\n",
    "# reload options\n",
    "if model_options['reload_'] and os.path.exists(model_options['saveto']):\n",
    "    print 'Reloading model options'\n",
    "    with open('%s.pkl' % model_options['saveto'], 'rb') as f:\n",
    "        model_options = pkl.load(f)\n",
    "\n",
    "        model_options['overwrite']  = False\n",
    "        model_options['saveFreq']   = 500\n",
    "        model_options['sampleFreq'] = 20\n",
    "\n",
    "@Timeit\n",
    "def build_networks(options):\n",
    "    funcs = dict()\n",
    "\n",
    "    print 'Building model: E -> F & F -> E model'\n",
    "    params_ef = init_params(options, 'ef_')\n",
    "    params_fe = init_params(options, 'fe_')\n",
    "    print 'Done.'\n",
    "\n",
    "    # reload parameters\n",
    "    if options['reload_'] and os.path.exists(options['saveto']):\n",
    "        print 'Reloading model parameters'\n",
    "        params_ef = load_params(options['saveto'], params_ef)\n",
    "        params_fe = load_params(options['saveto'], params_fe)\n",
    "\n",
    "    tparams_ef = init_tparams(params_ef)\n",
    "    tparams_fe = init_tparams(params_fe)\n",
    "\n",
    "    # inputs of the model (x1, y1, x2, y2)\n",
    "    x1 = tensor.matrix('x1', dtype='int64')\n",
    "    x1_mask = tensor.matrix('x1_mask', dtype='float32')\n",
    "    y1 = tensor.matrix('y1', dtype='int64')\n",
    "    y1_mask = tensor.matrix('y1_mask', dtype='float32')\n",
    "    x2 = tensor.matrix('x2', dtype='int64')\n",
    "    x2_mask = tensor.matrix('x2_mask', dtype='float32')\n",
    "    y2 = tensor.matrix('y2', dtype='int64')\n",
    "    y2_mask = tensor.matrix('y2_mask', dtype='float32')\n",
    "\n",
    "    # TM reference index\n",
    "    tef12 = tensor.matrix('ef12', dtype='int64')\n",
    "    tef12_mask = tensor.matrix('ef12_mask', dtype='float32')\n",
    "    tef21 = tensor.matrix('ef21', dtype='int64')\n",
    "    tef21_mask = tensor.matrix('ef21_mask', dtype='float32')\n",
    "    tfe12 = tensor.matrix('fe12', dtype='int64')\n",
    "    tfe12_mask = tensor.matrix('fe12_mask', dtype='float32')\n",
    "    tfe21 = tensor.matrix('fe21', dtype='int64')\n",
    "    tfe21_mask = tensor.matrix('fe21_mask', dtype='float32')\n",
    "\n",
    "    print 'build forward-attention models (4 models simultaneously)...'\n",
    "    ret_ef11 = build_model(tparams_ef, [x1, x1_mask, y1, y1_mask], options, 'ef_', False, True)   # E->F curr\n",
    "    ret_fe11 = build_model(tparams_fe, [y1, y1_mask, x1, x1_mask], options, 'fe_', False, False)  # F->E curr\n",
    "    ret_ef22 = build_model(tparams_ef, [x2, x2_mask, y2, y2_mask], options, 'ef_', False, True)   # E->F tm\n",
    "    ret_fe22 = build_model(tparams_fe, [y2, y2_mask, x2, x2_mask], options, 'fe_', False, False)  # F->E tm\n",
    "\n",
    "    print 'build cross-attention models'\n",
    "    ret_ef12 = build_attender(tparams_ef,\n",
    "                              [ret_ef11['prev_hids'], ret_ef11['prev_emb'], ret_ef22['ctx'], x2_mask],\n",
    "                              options, 'ef_')  # E->F curr\n",
    "    ret_ef21 = build_attender(tparams_ef,\n",
    "                              [ret_ef22['prev_hids'], ret_ef22['prev_emb'], ret_ef11['ctx'], x1_mask],\n",
    "                              options, 'ef_')  # E->F tm\n",
    "    ret_fe12 = build_attender(tparams_fe,\n",
    "                              [ret_fe11['prev_hids'], ret_fe11['prev_emb'], ret_fe22['ctx'], y2_mask],\n",
    "                              options, 'fe_')  # F->E curr\n",
    "    ret_fe21 = build_attender(tparams_fe,\n",
    "                              [ret_fe22['prev_hids'], ret_fe22['prev_emb'], ret_fe11['ctx'], y1_mask],\n",
    "                              options, 'fe_')  # F->E tm\n",
    "\n",
    "    print 'build attentions (forward, cross-propagation)'\n",
    "\n",
    "    def build_prop(atten_ef, atten_fe):\n",
    "        atten_ef = atten_ef.dimshuffle(1, 0, 2)\n",
    "        atten_fe = atten_fe.dimshuffle(1, 0, 2)\n",
    "        attention = tensor.batched_dot(atten_ef, atten_fe).dimshuffle(1, 0, 2)\n",
    "        return attention\n",
    "\n",
    "    att_ef12 = build_prop(ret_ef12['attention'], ret_fe22['attention'])\n",
    "    att_ef21 = build_prop(ret_ef21['attention'], ret_fe11['attention'])\n",
    "    att_fe12 = build_prop(ret_fe12['attention'], ret_ef22['attention'])\n",
    "    att_fe21 = build_prop(ret_fe21['attention'], ret_ef11['attention'])\n",
    "\n",
    "    print 'build gates!'\n",
    "    params_gate  = OrderedDict()\n",
    "    params_gate  = get_layer('bi')[0](options, params_gate, nin=2 * options['dim'])\n",
    "    tparams_gate = init_tparams(params_gate)\n",
    "\n",
    "    # a neural gate which is the relatedness of two attentions.\n",
    "    # def build_gate(ctx1, ctx2):\n",
    "    #     return get_layer('bi')[1](tparams_gate, ctx1, ctx2)\n",
    "    #\n",
    "    # gate_ef1 = 1 - build_gate(ret_ef11['ctxs'], ret_ef12['ctxs'])\n",
    "    # gate_ef2 = 1 - build_gate(ret_ef22['ctxs'], ret_ef21['ctxs'])\n",
    "    # gate_fe1 = 1 - build_gate(ret_fe11['ctxs'], ret_fe12['ctxs'])\n",
    "    # gate_fe2 = 1 - build_gate(ret_fe22['ctxs'], ret_fe21['ctxs'])\n",
    "    #\n",
    "    # print 'Building Gate functions, ...',\n",
    "    # f_gate = theano.function([ret_ef11['ctxs'], ret_ef12['ctxs']],\n",
    "    #                           gate_ef1, profile=profile)\n",
    "    # print 'Done.'\n",
    "\n",
    "    print 'Building a Natural Gate Function'\n",
    "    gate_ef1 = 1 - tensor.clip(ret_ef12['att_sum'] / (ret_ef11['att_sum']), 0, 1)\n",
    "    gate_ef2 = 1 - tensor.clip(ret_ef21['att_sum'] / (ret_ef22['att_sum']), 0, 1)\n",
    "    gate_fe1 = 1 - tensor.clip(ret_fe12['att_sum'] / (ret_fe11['att_sum']), 0, 1)\n",
    "    gate_fe2 = 1 - tensor.clip(ret_fe21['att_sum'] / (ret_fe22['att_sum']), 0, 1)\n",
    "\n",
    "    print 'build loss function (w/o gate)'\n",
    "\n",
    "    # get the loss function\n",
    "    def compute_prob(probs, y, y_mask):\n",
    "\n",
    "        # compute the loss for the vocabulary-selection side\n",
    "        y_flat  = y.flatten()\n",
    "        n_words = probs.shape[-1]\n",
    "        y_flat_idx = tensor.arange(y_flat.shape[0]) * n_words + y_flat\n",
    "        probw   = probs.flatten()[y_flat_idx]\n",
    "        probw   = probw.reshape([y.shape[0], y.shape[1]]) * y_mask\n",
    "        return probw\n",
    "\n",
    "    prob_ef11 = ret_ef11['probs']\n",
    "    prob_ef22 = ret_ef22['probs']\n",
    "    prob_fe11 = ret_fe11['probs']\n",
    "    prob_fe22 = ret_fe22['probs']\n",
    "\n",
    "    def compute_cost(prob, y, y_mask, att, t, t_mask, g):\n",
    "        _y = tensor.eq(y, 1)\n",
    "        y_mask *= ((1 - _y) + _y * (1 - t_mask))\n",
    "        ccost = -tensor.log(compute_prob(prob, y, y_mask) * g +\n",
    "                            compute_prob(att, t, t_mask) * (1 - g) +\n",
    "                            1e-7)\n",
    "        ccost = (ccost * (1 - (1 - y_mask) * (1 - t_mask))).sum(0)\n",
    "        return ccost\n",
    "\n",
    "    # get cost\n",
    "    cost_ef1 = compute_cost(prob_ef11, y1, y1_mask, att_ef12, tef12, tef12_mask, gate_ef1)\n",
    "    cost_ef2 = compute_cost(prob_ef22, y2, y2_mask, att_ef21, tef21, tef21_mask, gate_ef2)\n",
    "    cost_fe1 = compute_cost(prob_fe11, x1, x1_mask, att_fe12, tfe12, tfe12_mask, gate_fe1)\n",
    "    cost_fe2 = compute_cost(prob_fe22, x2, x2_mask, att_fe21, tfe21, tfe21_mask, gate_fe2)\n",
    "\n",
    "    cost = cost_ef1 + cost_ef2 + cost_fe1 + cost_fe2\n",
    "\n",
    "    print 'build sampler (one-step)'\n",
    "    f_init_ef, f_next_ef = build_sampler(tparams_ef, options, options['trng'], 'ef_')\n",
    "    f_init_fe, f_next_fe = build_sampler(tparams_fe, options, options['trng'], 'fe_')\n",
    "\n",
    "    print 'build attender (one-step)'\n",
    "    f_attend_ef = build_attender(tparams_ef, None, options, 'ef_', one_step=True)  # E->F curr\n",
    "    f_attend_fe = build_attender(tparams_fe, None, options, 'fe_', one_step=True)\n",
    "\n",
    "    # before any regularizer\n",
    "    print 'build Cost Function...',\n",
    "    inputs = [x1, x1_mask, y1, y1_mask, x2, x2_mask, y2, y2_mask,\n",
    "              tef12, tef12_mask, tef21, tef21_mask,\n",
    "              tfe12, tfe12_mask, tfe21, tfe21_mask]\n",
    "    f_valid = theano.function(inputs, cost, profile=profile)\n",
    "\n",
    "    print 'build Gradient (backward)...',\n",
    "    cost    = cost.mean()\n",
    "    # tparams = dict(tparams_ef.items() + tparams_fe.items() + tparams_gate.items())\n",
    "    tparams = dict(tparams_ef.items() + tparams_fe.items())\n",
    "    grads   = clip(tensor.grad(cost, wrt=itemlist(tparams)), options['clip_c'])\n",
    "    print 'Done'\n",
    "\n",
    "    # compile the optimizer, the actual computational graph is compiled here\n",
    "    lr = tensor.scalar(name='lr')\n",
    "    print 'Building Optimizers...',\n",
    "    f_cost, f_update = eval(options['optimizer'])(lr, tparams, grads, inputs, cost)\n",
    "\n",
    "    print 'Done'\n",
    "\n",
    "    # put everything into function lists\n",
    "    funcs['valid']  = f_valid\n",
    "    funcs['cost']   = f_cost\n",
    "    funcs['update'] = f_update\n",
    "\n",
    "    funcs['init_ef'] = f_init_ef\n",
    "    funcs['init_fe'] = f_init_fe\n",
    "    funcs['next_ef'] = f_next_ef\n",
    "    funcs['next_fe'] = f_next_fe\n",
    "\n",
    "    funcs['att_ef']  = f_attend_ef\n",
    "    funcs['att_fe']  = f_attend_fe\n",
    "\n",
    "    funcs['crit_ef'] = ret_ef11['f_critic']\n",
    "    funcs['crit_fe'] = ret_ef22['f_critic']\n",
    "\n",
    "    # funcs['gate']    = f_gate\n",
    "\n",
    "    print 'Build Networks... done!'\n",
    "    return funcs, tparams\n",
    "\n",
    "funcs, tparams = build_networks(model_options)\n",
    "\n",
    "# print 'save the compiled functions/tparams for temperal usage'\n",
    "\n",
    "\n",
    "print 'Loading data'\n",
    "train = TextIterator(model_options['datasets'], model_options['dictionaries'], [0, 0, 0, 0],\n",
    "                     batch_size=model_options['batch_size'], maxlen=model_options['maxlen'])\n",
    "valid = TextIterator(model_options['valid_datasets'], model_options['dictionaries'], [0, 0, 0, 0],\n",
    "                     batch_size=model_options['batch_size'], maxlen=200)\n",
    "\n",
    "if model_options['use_pretrain']:\n",
    "    print 'use the pretrained NMT-models...',\n",
    "    params = unzip(tparams)\n",
    "    params = load_params2(model_options['baseline_ef'], params, mode='ef_')\n",
    "    params = load_params2(model_options['baseline_fe'], params, mode='fe_')\n",
    "    zipp(params, tparams)\n",
    "    print 'Done.'\n",
    "\n",
    "else:\n",
    "    print 'not loading the pretrained baseline'\n",
    "\n",
    "print '-------------------------------------------- Main-Loop -------------------------------------------------'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_sample(tparams,\n",
    "               funcs,\n",
    "               x1, x2, y2,\n",
    "               options,\n",
    "               rng=None,\n",
    "               m=0,\n",
    "               k=1,  # beam-size\n",
    "               maxlen=200,\n",
    "               stochastic=True,\n",
    "               argmax=False):\n",
    "    # modes\n",
    "    modes = ['ef', 'fe']\n",
    "\n",
    "    # masks\n",
    "    x1_mask = numpy.array(x1 > 0, dtype='float32')\n",
    "    x2_mask = numpy.array(x2 > 0, dtype='float32')\n",
    "    y2_mask = numpy.array(y2 > 0, dtype='float32')\n",
    "\n",
    "    # k is the beam size we have\n",
    "    if k > 1:\n",
    "        assert not stochastic, 'Beam search does not support stochastic sampling'\n",
    "\n",
    "    sample = []\n",
    "    action = []\n",
    "    sample_score = []\n",
    "    if stochastic:\n",
    "        sample_score = 0\n",
    "\n",
    "    live_k = 1\n",
    "    dead_k = 0\n",
    "\n",
    "    hyp_samples = [[]] * live_k\n",
    "    hyp_actions = [[]] * live_k\n",
    "    hyp_scores  = numpy.zeros(live_k).astype('float32')\n",
    "    hyp_states  = []\n",
    "\n",
    "    # get initial state of decoder rnn and encoder context for x1\n",
    "    ret = funcs['init_' + modes[m]](x1)\n",
    "    next_state, ctx0 = ret[0], ret[1]  # init-state, contexts\n",
    "    next_w = -1 * numpy.ones((1,)).astype('int64')  # bos indicator\n",
    "\n",
    "    # get translation memory encoder context\n",
    "    _, mctx0 = funcs['init_' + modes[m]](x2)\n",
    "\n",
    "    # get attention propagation for translation memory\n",
    "    attpipe, _ = funcs['crit_' + modes[1 - m]](y2, y2_mask, x2, x2_mask)\n",
    "    attpipe = numpy.squeeze(attpipe)\n",
    "\n",
    "    for ii in xrange(maxlen):\n",
    "        ctx = numpy.tile(ctx0, [live_k, 1])\n",
    "        mctx = numpy.tile(mctx0, [live_k, 1])\n",
    "\n",
    "        # --copy mode\n",
    "        ret = funcs['att_' + modes[m]](next_state, next_w, mctx)\n",
    "        mctxs, matt, mattsum = ret[0], ret[1], ret[2]    # matt: batchsize x len_x2\n",
    "        copy_p = numpy.dot(matt, attpipe)  # batchsize x len_y2\n",
    "\n",
    "        # --generate mode\n",
    "        ret = funcs['next_' + modes[m]](next_w, ctx, next_state)\n",
    "        next_p, next_w, next_state, ctxs, attsum = ret[0], ret[1], ret[2], ret[3], ret[4]\n",
    "\n",
    "        # compute gate\n",
    "        # gates = funcs['gate'](ctxs[None, :, :], mctxs[None, :, :])[0]  # batchsize\n",
    "        # gates = numpy.clip(mattsum / (mattsum + attsum), 0, 1) # Natural Gate.\n",
    "        gates = numpy.clip(mattsum / attsum, 0, 1)\n",
    "        \n",
    "        # real probabilities\n",
    "        next_p *= (1 - gates[:, None])\n",
    "        copy_p *= gates[:, None]\n",
    "\n",
    "        def _merge():\n",
    "            temp_p = copy.copy(numpy.concatenate([next_p, copy_p], axis=1))\n",
    "            lmax = next_p.shape[1]\n",
    "            for i in range(next_p.shape[0]):\n",
    "                for j in range(copy_p.shape[1]):\n",
    "                    if y2[j] != 1:\n",
    "                        temp_p[i, y2[j]] += copy_p[i, j]\n",
    "                        temp_p[i, lmax + j] = 0.\n",
    "                temp_p[i, 1] = 0. # never output UNK\n",
    "            # temp_p -= 1e-8\n",
    "            return temp_p\n",
    "\n",
    "        merge_p = _merge()\n",
    "\n",
    "        if stochastic:\n",
    "            if argmax:\n",
    "                nw = merge_p[0].argmax()\n",
    "                next_w[0] = nw\n",
    "            else:\n",
    "                nw = rng.multinomial(1, pvals=merge_p[0]).argmax()\n",
    "\n",
    "            sample.append(nw)\n",
    "            action.append(gates[0])\n",
    "            sample_score -= numpy.log(merge_p[0, nw])\n",
    "            if nw == 0:\n",
    "                break\n",
    "\n",
    "\n",
    "    return sample, sample_score, action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------- Main-Loop -------------------------------------------------\n",
      "Loading data\n",
      "Source-CR 0: Pour la réexportation des marchandises non communautaires qui ne sont pas déchargées ou qui sont transbordées au sens de l &apos; article 176 paragraphe 2 du code , la notification visée à l &apos; article 182 paragraphe 3 du code n &apos; est pas nécessaire . &quot;\n",
      "Target-CR 0: In the case of the re-exportation of non-Community goods , which are not unloaded or which are transhipped within the meaning of Article 176 ( 2 ) of the Code , the notification referred to in Article 182 ( 3 ) of the Code shall not be required . &apos;\n",
      "-----------------------------\n",
      "Source-TM 0: Pour la réexportation des marchandises non communautaires qui ne sont pas déchargées ou qui sont transbordées au sens de l &apos; article 176 paragraphe 2 du code , la notification visée à l &apos; article 182 paragraphe 3 du code n &apos; est pas nécessaire . &quot;\n",
      "Target-TM 0: In the case of the re-exportation of non-Community goods , which are not unloaded or which are transhipped within the meaning of Article 176 ( 2 ) of the Code , the notification referred to in Article 182 ( 3 ) of the Code shall not be required . &apos;\n",
      "=============================\n",
      "Sample-CR 0: In In re-exportation of non-Community goods , , , , , , , notification within Article 182 ( 2 3 2 of Code , notification referred in Article 182 ( 3 3 of Code shall not required . &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos;\n",
      "Copy Prob 0: 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00\n",
      "\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "123",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 123\n"
     ]
    }
   ],
   "source": [
    "print '-------------------------------------------- Main-Loop -------------------------------------------------'\n",
    "\n",
    "# ------------------ initlization --------------- #\n",
    "best_p       = None\n",
    "bad_counter  = 0\n",
    "uidx         = 0\n",
    "estop        = False\n",
    "history_errs = []\n",
    "max_epochs   = 100\n",
    "finish_after = 10000000\n",
    "\n",
    "lrate        = model_options['lrate']\n",
    "saveFreq     = model_options['saveFreq']\n",
    "sampleFreq   = model_options['sampleFreq']\n",
    "validFreq    = model_options['validFreq']\n",
    "saveto       = model_options['saveto']\n",
    "overwrite    = model_options['overwrite']\n",
    "\n",
    "# ----------------------------------------------- #\n",
    "\n",
    "# reload history\n",
    "if model_options['reload_'] and os.path.exists(model_options['saveto']):\n",
    "    rmodel = numpy.load(model_options['saveto'])\n",
    "    history_errs = list(rmodel['history_errs'])\n",
    "    if 'uidx' in rmodel:\n",
    "        uidx = rmodel['uidx']\n",
    "\n",
    "\n",
    "# idx back to sequences\n",
    "def idx2seq(x, ii):\n",
    "    seq = []\n",
    "    for vv in x:\n",
    "        if vv == 0:\n",
    "            break\n",
    "        if vv in worddicts_r[ii]:\n",
    "            seq.append(worddicts_r[ii][vv])\n",
    "        else:\n",
    "            seq.append('UNK')\n",
    "    return ' '.join(seq)\n",
    "\n",
    "\n",
    "# compute-update\n",
    "@Timeit\n",
    "def execute(inps, lrate, info):\n",
    "    eidx, uidx = info\n",
    "    cost = funcs['cost'](*inps)\n",
    "\n",
    "    # check for bad numbers, usually we remove non-finite elements\n",
    "    # and continue training - but not done here\n",
    "    if numpy.isnan(cost) or numpy.isinf(cost):\n",
    "        print 'NaN detected'\n",
    "        sys.exit(-1)\n",
    "\n",
    "    funcs['update'](lrate)\n",
    "    print 'Epoch ', eidx, 'Update ', uidx, 'Cost ', cost,\n",
    "    return cost\n",
    "\n",
    "def prepare_cross2(seqs_x1, seqs_x2, maxlen_x1):\n",
    "    n_samples = len(seqs_x1)\n",
    "    t = numpy.zeros((maxlen_x1, n_samples)).astype('int64')\n",
    "    t_mask = numpy.zeros((maxlen_x1, n_samples)).astype('float32')\n",
    "\n",
    "    for idx, (x1, x2) in enumerate(zip(seqs_x1, seqs_x2)):\n",
    "\n",
    "        match = [[(i, abs(i - j))\n",
    "                  for i, xx2 in enumerate(x2) if xx1 == xx2]\n",
    "                 for j, xx1 in enumerate(x1)]\n",
    "\n",
    "        for jdx, m in enumerate(match):\n",
    "            if len(m) > 0:\n",
    "                if len(m) == 1:\n",
    "                    t[jdx, idx] = m[0][0]\n",
    "                else:\n",
    "                    t[jdx, idx] = sorted(m, key=lambda a: a[1])[0][0]\n",
    "\n",
    "                t_mask[jdx, idx] = 1.\n",
    "\n",
    "    return t, t_mask\n",
    "\n",
    "# start!!\n",
    "print 'Loading data'\n",
    "train = TextIterator(model_options['datasets'], model_options['dictionaries'], [0, 0, 0, 0],\n",
    "                     batch_size=model_options['batch_size'], maxlen=model_options['maxlen'])\n",
    "valid = TextIterator(model_options['valid_datasets'], model_options['dictionaries'], [0, 0, 0, 0],\n",
    "                     batch_size=model_options['batch_size'], maxlen=200)\n",
    "\n",
    "\n",
    "max_epochs = 1\n",
    "for eidx in xrange(max_epochs):\n",
    "    n_samples = 0\n",
    "\n",
    "    for k, (sx1, sy1, sx2, sy2) in enumerate(train):\n",
    "        uidx += 1\n",
    "        \n",
    "        x1, x1_mask = prepare_data(sx1, model_options['maxlen'], model_options['voc_sizes'][0])\n",
    "        y1, y1_mask = prepare_data(sy1, model_options['maxlen'], model_options['voc_sizes'][1])\n",
    "        x2, x2_mask = prepare_data(sx2, model_options['maxlen'], model_options['voc_sizes'][2])\n",
    "        y2, y2_mask = prepare_data(sy2, model_options['maxlen'], model_options['voc_sizes'][3])\n",
    "\n",
    "        tx12, tx12_mask = prepare_cross2(sx1, sx2, x1.shape[0])\n",
    "        tx21, tx21_mask = prepare_cross2(sx2, sx1, x2.shape[0])\n",
    "        ty12, ty12_mask = prepare_cross2(sy1, sy2, y1.shape[0])\n",
    "        ty21, ty21_mask = prepare_cross2(sy1, sy2, y2.shape[0])\n",
    "\n",
    "        inps = [x1, x1_mask, y1, y1_mask,\n",
    "                x2, x2_mask, y2, y2_mask,\n",
    "                ty12, ty12_mask, ty21, ty21_mask,\n",
    "                tx12, tx12_mask, tx21, tx21_mask]\n",
    "\n",
    "       \n",
    "       \n",
    "        for jj in xrange(numpy.minimum(5, x1.shape[1])):\n",
    "            stochastic = True\n",
    "            sample, sc, acts = get_sample(tparams, funcs,\n",
    "                                       x1[:, jj][:, None],\n",
    "                                       x2[:, jj][:, None],\n",
    "                                       y2[:, jj][:, None],\n",
    "                                       model_options,\n",
    "                                       rng=model_options['rng'],\n",
    "                                       m=1,\n",
    "                                       k=1,\n",
    "                                       maxlen=200,\n",
    "                                       stochastic=model_options['stochastic'],\n",
    "                                       argmax=True)\n",
    "\n",
    "            print 'Source-CR {}: {}'.format(jj, idx2seq(sx1[jj], 0))\n",
    "            print 'Target-CR {}: {}'.format(jj, idx2seq(sy1[jj], 1))\n",
    "            print '-----------------------------'\n",
    "            print 'Source-TM {}: {}'.format(jj, idx2seq(sx2[jj], 2))\n",
    "            print 'Target-TM {}: {}'.format(jj, idx2seq(sy2[jj], 3))\n",
    "            print '============================='\n",
    "\n",
    "            if model_options['stochastic']:\n",
    "                ss = sample\n",
    "            else:\n",
    "                sc /= numpy.array([len(s) for s in sample])\n",
    "                ss = sample[sc.argmin()]\n",
    "\n",
    "            _ss = []\n",
    "            for ii, si in enumerate(ss):\n",
    "                if si < model_options['voc_sizes'][1]:\n",
    "                    _ss.append(si)\n",
    "                else:\n",
    "                    print si\n",
    "                    offset = si - model_options['voc_sizes'][1]\n",
    "                    _ss.append(sy2[jj][offset])\n",
    "\n",
    "            print 'Sample-CR {}: {}'.format(jj, idx2seq(_ss, 1))\n",
    "            print 'Copy Prob {}: {}'.format(jj, ' '.join(['{:.2f}'.format(a) for a in acts]))\n",
    "            print\n",
    "\n",
    "            import sys; sys.exit(123)\n",
    "            break\n",
    "        \n",
    "        \n",
    "    print 'Seen %d samples' % n_samples\n",
    "\n",
    "    if estop:\n",
    "        break\n",
    "\n",
    "if best_p is not None:\n",
    "    zipp(best_p, tparams)\n",
    "\n",
    "valid_err = validate(funcs, model_options, valid).mean()\n",
    "print 'Valid ', valid_err\n",
    "\n",
    "params = copy.copy(best_p)\n",
    "numpy.savez(saveto, zipped_params=best_p,\n",
    "            history_errs=history_errs,\n",
    "            uidx=uidx,\n",
    "            **params)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gate=1\n",
    "-------------------------------------------- Main-Loop -------------------------------------------------\n",
    "Loading data\n",
    "Source-CR 0: Pour la réexportation des marchandises non communautaires qui ne sont pas déchargées ou qui sont transbordées au sens de l &apos; article 176 paragraphe 2 du code , la notification visée à l &apos; article 182 paragraphe 3 du code n &apos; est pas nécessaire . &quot;\n",
    "Target-CR 0: In the case of the re-exportation of non-Community goods , which are not unloaded or which are transhipped within the meaning of Article 176 ( 2 ) of the Code , the notification referred to in Article 182 ( 3 ) of the Code shall not be required . &apos;\n",
    "-----------------------------\n",
    "Source-TM 0: Pour la réexportation des marchandises non communautaires qui ne sont pas déchargées ou qui sont transbordées au sens de l &apos; article 176 paragraphe 2 du code , la notification visée à l &apos; article 182 paragraphe 3 du code n &apos; est pas nécessaire . &quot;\n",
    "Target-TM 0: In the case of the re-exportation of non-Community goods , which are not unloaded or which are transhipped within the meaning of Article 176 ( 2 ) of the Code , the notification referred to in Article 182 ( 3 ) of the Code shall not be required . &apos;\n",
    "=============================\n",
    "Sample-CR 0: In In re-exportation of non-Community goods , , , , , , , notification within Article 182 ( 2 3 2 of Code , notification referred in Article 182 ( 3 3 of Code shall not required . &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos; &apos;\n",
    "Copy Prob 0: 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gate = 0\n",
    "-------------------------------------------- Main-Loop -------------------------------------------------\n",
    "Loading data\n",
    "Source-CR 0: Pour la réexportation des marchandises non communautaires qui ne sont pas déchargées ou qui sont transbordées au sens de l &apos; article 176 paragraphe 2 du code , la notification visée à l &apos; article 182 paragraphe 3 du code n &apos; est pas nécessaire . &quot;\n",
    "Target-CR 0: In the case of the re-exportation of non-Community goods , which are not unloaded or which are transhipped within the meaning of Article 176 ( 2 ) of the Code , the notification referred to in Article 182 ( 3 ) of the Code shall not be required . &apos;\n",
    "-----------------------------\n",
    "Source-TM 0: Pour la réexportation des marchandises non communautaires qui ne sont pas déchargées ou qui sont transbordées au sens de l &apos; article 176 paragraphe 2 du code , la notification visée à l &apos; article 182 paragraphe 3 du code n &apos; est pas nécessaire . &quot;\n",
    "Target-TM 0: In the case of the re-exportation of non-Community goods , which are not unloaded or which are transhipped within the meaning of Article 176 ( 2 ) of the Code , the notification referred to in Article 182 ( 3 ) of the Code shall not be required . &apos;\n",
    "=============================\n",
    "Sample-CR 0: For the re-exportation of non-Community goods which are not unloaded or transhipped within the meaning of Article 176 ( 2 ) of the Code , the notification provided for in Article 182 ( 3 ) of the Code shall be required . &apos; ;\n",
    "Copy Prob 0: 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00\n",
    "\n",
    "An exception has occurred, use %tb to see the full traceback."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gate = Natural\n",
    "-------------------------------------------- Main-Loop -------------------------------------------------\n",
    "Loading data\n",
    "Source-CR 0: Pour la réexportation des marchandises non communautaires qui ne sont pas déchargées ou qui sont transbordées au sens de l &apos; article 176 paragraphe 2 du code , la notification visée à l &apos; article 182 paragraphe 3 du code n &apos; est pas nécessaire . &quot;\n",
    "Target-CR 0: In the case of the re-exportation of non-Community goods , which are not unloaded or which are transhipped within the meaning of Article 176 ( 2 ) of the Code , the notification referred to in Article 182 ( 3 ) of the Code shall not be required . &apos;\n",
    "-----------------------------\n",
    "Source-TM 0: Pour la réexportation des marchandises non communautaires qui ne sont pas déchargées ou qui sont transbordées au sens de l &apos; article 176 paragraphe 2 du code , la notification visée à l &apos; article 182 paragraphe 3 du code n &apos; est pas nécessaire . &quot;\n",
    "Target-TM 0: In the case of the re-exportation of non-Community goods , which are not unloaded or which are transhipped within the meaning of Article 176 ( 2 ) of the Code , the notification referred to in Article 182 ( 3 ) of the Code shall not be required . &apos;\n",
    "=============================\n",
    "Sample-CR 0: In the case of re-exportation of non-Community goods , which are not unloaded or transhipped within the meaning of Article 176 ( 2 ) of the Code , the notification referred to in Article 182 ( 3 ) of the Code shall be required . &apos; ;\n",
    "Copy Prob 0: 0.50 0.50 0.50 0.50 0.50 0.50 0.50 0.50 0.50 0.50 0.50 0.50 0.50 0.50 0.50 0.50 0.50 0.50 0.50 0.50 0.50 0.50 0.50 0.50 0.50 0.50 0.50 0.50 0.50 0.50 0.50 0.50 0.50 0.50 0.50 0.50 0.50 0.50 0.50 0.50 0.50 0.50 0.50 0.50 0.50 0.50 0.50 0.50\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  17 1756   12  116  163    2 3814   61    3   18   87    2 6001    2 2139\n",
      "    4    2 2139   26    2 1581    2    8    3  948    4   25  145  764 2517\n",
      "   50  874    4   81    9    8    3   30  131    4  365  152   11  140    4\n",
      "   15   39    2  124    7    0] (51,)\n",
      "[   9 1693    8  181   62   15 2043   30   11    2  674    3   14 2825   20\n",
      " 1848    3    2  482   20   14 2895 2825    4   50  758  404   54   15  839\n",
      "    4  217    7   22  122    9  149    8   10    9  127    8    3    2  437\n",
      "   32    6    0    0    0    0] (51,)\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46  0  0  0\n",
      "  0]\n",
      "[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  0.  0.  0.  0.]\n",
      "[9, 1693, 8, 181, 62, 15, 2043, 30, 11, 2, 674, 3, 14, 2825, 20, 1848, 3, 2, 482, 20, 14, 2895, 2825, 4, 50, 758, 404, 54, 15, 839, 4, 217, 7, 22, 122, 9, 149, 8, 10, 9, 127, 8, 3, 2, 437, 32, 6]\n"
     ]
    }
   ],
   "source": [
    "print x1[:, 1], x1[:, 1].shape\n",
    "print y1[:, 1], y1[:, 1].shape\n",
    "print ty12[:, 1]\n",
    "print ty12_mask[:, 1]\n",
    "print sy1[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
